# -*- coding: utf-8 -*-
"""M25CSE012_prob4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PX8lugML_lmZYXLdqtByTbS-9FDBcV0K

PROBLEM 4: SPORTS OR POLITICS
"""

#Step1 :- Importing Libraries
import pandas as pd
import numpy as np
import math
import re
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

#Step 2:- Loading the datase we have used here BBC dataset here which contain articles on Sports and Politics
def data_loading(file_path):

    df = pd.read_csv(
        file_path,
        sep='\t',
        engine="python",
        encoding="utf-8",
        quotechar='"',
        on_bad_lines="skip"
    )

    print("Original Dataset Shape:", df.shape)
    print("Available Categories:", df["category"].unique())

    df = df[df["category"].isin(["sport", "politics"])]

    print("Shape Of Dataset after filtering is :", df.shape)

    df["text"] = df["title"] + " " + df["content"]

    return df[["text", "category"]]

#Step 3:- Preprocessing the data
def preprocessing(text):
#Here we convert the text to lowercase
    text = str(text).lower()
    text = re.sub(r'[^a-z\s]', '', text)
    return text.split()
def get_ngrams(tokens, n=2):
    #Here we generate the tokens for n-gram here
    return [" ".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

#Step 4:Exploratory Data Analysis on the dataset, like documnets legth , most common words per class and other things

def eda_analysis(df):


    #Here in head we see the first 5 data cloumns here
    print("\n===== DATA HEAD =====")
    print(df.head())

    # Class distribution of each category
    print("\nClass Distribution:")
    print(df['category'].value_counts())
    plt.figure(figsize=(6,4))
    sns.countplot(x='category', data=df)
    plt.title("Class Distribution")
    plt.show()

    #HEre we repesent the word count per documnet
    df['word_count'] = df['content'].apply(lambda x: len(preprocessing(x)))
    print("\nAverage Word Count Per Class:")
    print(df.groupby('category')['word_count'].mean())

    # Histogram of document length we generate here
    plt.figure(figsize=(8,5))
    sns.histplot(data=df, x='word_count', hue='category', bins=30, kde=True)
    plt.title("Document Length Distribution")
    plt.xlabel("Word Count")
    plt.ylabel("Number of Documents")
    plt.show()

    #Here we print the top 20 overall words that we have here
    all_words = [word for doc in df['content'] for word in preprocessing(doc)]
    common_words = Counter(all_words).most_common(20)
    print("\nTop 20 Most Common Words (Overall):")
    print(common_words)

    #HEre we print the top 20 words that we haver per class
    for cat in df['category'].unique():
        words_cat = [word for doc in df[df['category']==cat]['content'] for word in preprocessing(doc)]
        common_words_cat = Counter(words_cat).most_common(20)
        print(f"\nTop 20 Most Common Words ({cat}):")
        print(common_words_cat)

#Step 5:- We are defining  function here , for feature representation.
# We use: n-grams, TF-IDF, Bag of Words for feature represenatation.
class Extractingfeature:

  #Here we define custom TF-IDF + N-gram feature extractor.
  #n_gram_range that is defined here tuple (min_n, max_n), e.g., (1,2) for unigrams + bigrams

    def __init__(self, n_gram_range=(1,2)):
        self.n_gram_range = n_gram_range
        self.idf = {}
        self.vocab = []
        self.word_to_idx = {}

    def fit_transform(self, docs):
        total_docs = len(docs)
        processed_docs = []
        df_counts = Counter()

        # Step a: Here we generate N-grams & compute document frequency for each of the documnet that we have here
        for doc in docs:
            tokens = preprocessing(doc)
            features = []
            for n in range(self.n_gram_range[0], self.n_gram_range[1]+1):
                features.extend(tokens if n == 1 else get_ngrams(tokens, n))
            processed_docs.append(features)
            for f in set(features):
                df_counts[f] += 1

        # Step b: Here we build the  vocabulary & IDF
        self.vocab = sorted([f for f, count in df_counts.items() if count > 2])
        self.word_to_idx = {f:i for i,f in enumerate(self.vocab)}
        self.idf = {f: math.log(total_docs / (1 + df_counts[f])) for f in self.vocab}

        # Step c:Here we craete the  TF-IDF matrix
        X = np.zeros((total_docs, len(self.vocab)))
        for i, features in enumerate(processed_docs):
            tf = Counter(features)
            for f, count in tf.items():
                if f in self.word_to_idx:
                    X[i, self.word_to_idx[f]] = (count / len(features)) * self.idf[f]
        return X

#Step-6 Here we define all our machine models that we are going to use for documnet classification here
#We are using 3 models here for documnet classfication here SVM , NaiveBayes and Logistic Regression here
# MODEL a: Naive Bayes
class NaiveBayesAlgo:
    def fit(self, X, y):
        self.classes = np.unique(y)
        self.class_priors = {c: np.mean(y == c) for c in self.classes}
        self.word_probs = {}
        for c in self.classes:
            self.word_probs[c] = (X[y==c].sum(axis=0) + 1) / (X[y==c].sum() + X.shape[1])

    def predict(self, X):
        preds = []
        for x in X:
            posteriors = []
            for c in self.classes:
                prior = np.log(self.class_priors[c])
                likelihood = np.sum(np.log(self.word_probs[c]) * x)
                posteriors.append(prior + likelihood)
            preds.append(self.classes[np.argmax(posteriors)])
        return np.array(preds)

# MODEL b: Logistic Regression
class LogisticRegressionAlgo:
    def __init__(self, lr=0.1, iters=500):
        self.lr = lr
        self.iters = iters

    def fit(self, X, y):
        y_bin = np.where(y == 'sport', 1, 0)
        self.w = np.zeros(X.shape[1])
        for _ in range(self.iters):
            z = np.dot(X, self.w)
            h = 1 / (1 + np.exp(-np.clip(z, -250, 250)))
            grad = np.dot(X.T, (h - y_bin)) / y_bin.size
            self.w -= self.lr * grad

    def predict(self, X):
        return np.where(1/(1+np.exp(-np.dot(X, self.w))) >= 0.5, 'sport', 'politics')

# MODEL c: SVM
class SVMAlgo:
    def __init__(self, lr=0.001, lambda_p=0.01, iters=500):
        self.lr = lr
        self.lambda_p = lambda_p
        self.iters = iters

    def fit(self, X, y):
        y_svm = np.where(y == 'sport', 1, -1)
        self.w = np.zeros(X.shape[1])
        for _ in range(self.iters):
            for i, x_i in enumerate(X):
                if y_svm[i] * np.dot(x_i, self.w) < 1:
                    self.w += self.lr * (x_i * y_svm[i] - 2 * self.lambda_p * self.w)
                else:
                    self.w -= self.lr * (2 * self.lambda_p * self.w)

    def predict(self, X):
        return np.where(np.dot(X, self.w) >= 0, 'sport', 'politics')

#Step7:- Here we perform the evaluation of the model here we calculate the accuracy, precision , recall, F1-score and all the metrics of the model here
def model_evaluation(y_true, y_pred, model_name):

    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, pos_label='sport')
    rec = recall_score(y_true, y_pred, pos_label='sport')
    f1 = f1_score(y_true, y_pred, pos_label='sport')

    print(f"\n==== {model_name} Evaluation ====")
    print(f"Accuracy : {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall   : {rec:.4f}")
    print(f"F1-Score : {f1:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred))

    #Confusion matrix of all the machine learning models
    cm = confusion_matrix(y_true, y_pred, labels=['sport', 'politics'])
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['sport', 'politics'],
                yticklabels=['sport', 'politics'])
    plt.title(f"{model_name} Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

#step 8:- We perform the entire process we have defined  the functions for all the teask that we have now we call those functions and generate the result here
def main_function():

    # Load dataset
    df = pd.read_csv('bbc_news.csv', sep='\t', on_bad_lines='skip', quoting=3)
    df = df[df['category'].isin(['sport', 'politics'])].reset_index(drop=True)
    eda_analysis(df)



    # Feature extraction
    print("\nExtracting Features (Unigrams + Bigrams + TF-IDF)...")
    fe = Extractingfeature(n_gram_range=(1,2))
    X = fe.fit_transform(df['content'].tolist())
    y = df['category'].values

    # Train-Test Split
    idx = np.random.permutation(len(y))
    split = int(0.8 * len(y))
    X_train, X_test = X[idx[:split]], X[idx[split:]]
    y_train, y_test = y[idx[:split]], y[idx[split:]]

    # Initialize models
    models = {
        "Naive Bayes": NaiveBayesAlgo(),
        "Logistic Regression": LogisticRegressionAlgo(),
        "SVM": SVMAlgo()
    }

    results = {}

    for name, model in models.items():
        print(f"\n=== Training {name} ===")
        model.fit(X_train, y_train)
        preds = model.predict(X_test)
        results[name] = np.mean(preds == y_test) * 100
        model_evaluation(y_test, preds, name)

    # Accuracy Comparison Plot
    plt.figure(figsize=(8,5))
    plt.bar(results.keys(), results.values(), color=['skyblue','salmon','lightgreen'])
    plt.ylim(0,100)
    plt.ylabel("Accuracy (%)")
    plt.title("Scratch Models Comparison (N-gram + TF-IDF)")
    plt.show()

    print("\nResults Table:")
    for m, a in results.items():
        print(f"{m}: {a:.2f}%")

if __name__ == "__main__":
    main_function()

